[1mdiff --git a/src/configs/cnn_lstm.ini b/src/configs/cnn_lstm.ini[m
[1mindex 6634508..d4b77b8 100644[m
[1m--- a/src/configs/cnn_lstm.ini[m
[1m+++ b/src/configs/cnn_lstm.ini[m
[36m@@ -11,4 +11,4 @@[m [mword embedding dimension = 100[m
 character embeddings dimension = 50[m
 max word length = 20[m
 lstm hidden state dim = 128[m
[31m-batch size = 64[m
[32m+[m[32mbatch size = 128[m
[1mdiff --git a/src/dataset_parser.py b/src/dataset_parser.py[m
[1mindex b9b84fe..213f09d 100644[m
[1m--- a/src/dataset_parser.py[m
[1m+++ b/src/dataset_parser.py[m
[36m@@ -18,7 +18,7 @@[m [mdef clear_tweet(tweet):[m
     """[m
     tweet = tweet[m
     for word in tweet.split():[m
[31m-        if word.startswith('#') or word.startswith('@') or word.startswith([m
[32m+[m[32m        if word.startswith('@') or word.startswith([m
                 '.@') or word.startswith('http'):[m
             tweet = tweet.replace(' ' + word, "")  # if it is on the end[m
             tweet = tweet.replace(word + ' ', "")  # if it is on the begining[m
[36m@@ -58,26 +58,6 @@[m [mdef loadGlove(glove_file):[m
     return embed_dict[m
 [m
 [m
[31m-def filterText(tweets):[m
[31m-    """[m
[31m-     Removes unnecessary data from the tweet such as extra hashtags, links...[m
[31m-    :param tweets: List of all tweets[m
[31m-    :return:[m
[31m-    """[m
[31m-    result = [][m
[31m-    for tweet in tweets:[m
[31m-        filtered = [][m
[31m-        for token in tweet:[m
[31m-            if (not (token.startswith('#') or token.startswith([m
[31m-                    '@') or token.startswith('.@') or token.startswith([m
[31m-                'http'))):[m
[31m-                filtered.append(token)[m
[31m-        while filtered.__contains__(''):[m
[31m-            filtered.remove('')[m
[31m-        result.append(filtered)[m
[31m-    return result[m
[31m-[m
[31m-[m
 def read_file_by_line_and_tokenize(file_path):[m
     """[m
     Reads the twee document file and tokenizes it.[m
[36m@@ -164,7 +144,7 @@[m [mdef createGlovefromTweet(embed_dict, tweetText, embedding_dim=100,[m
     :param tweetText: Tweeter text[m
 [m
     """[m
[31m-    tweetText=clear_tweet(tweetText)[m
[32m+[m[32m    tweetText = clear_tweet(tweetText)[m
     tokens = nltk.word_tokenize(tweetText)[m
     tokens = [word.lower() for word in tokens][m
     sentenceRow = np.zeros((embedding_dim, timestep))[m
[1mdiff --git a/src/models.py b/src/models.py[m
[1mindex 955f550..6bcca6c 100644[m
[1m--- a/src/models.py[m
[1m+++ b/src/models.py[m
[36m@@ -445,9 +445,8 @@[m [mclass CNN_BILST_FC(Net):[m
                 maxval=np.sqrt(3 / self.char_embedding_dim)),[m
             name="char_embedding")[m
 [m
[31m-        # TODO fix this by seperating layers[m
         net = tf.nn.embedding_lookup(char_embed, self.chr_embedding_input)[m
[31m-        net = slim.dropout(net, keep_prob=0.5, scope="dropout1")[m
[32m+[m[32m        net = slim.dropout(net, keep_prob=0.15, scope="dropout1")[m
         net = tf.expand_dims(net, axis=3)[m
 [m
         # Network layers[m
[36m@@ -497,14 +496,14 @@[m [mclass CNN_BILST_FC(Net):[m
 [m
         # Linear activation, using rnn inner loop on the final output[m
         net_rnn = slim.flatten(slim.dropout([m
[31m-            tf.matmul(net[-1], weights['out']) + biases['out'], keep_prob=0.5))[m
[32m+[m[32m            tf.matmul(net[-1], weights['out']) + biases['out'], keep_prob=0.15))[m
 [m
         # Merge CNN and RNN features[m
         net = tf.concat([net_cnn, net_rnn], axis=1)[m
 [m
         # FC layers[m
         net = slim.fully_connected(net, 512, scope='fc3')[m
[31m-        net = slim.dropout(net, keep_prob=0.5, scope="dropout4")[m
[32m+[m[32m        net = slim.dropout(net, keep_prob=0.15, scope="dropout4")[m
 [m
         logits = slim.fully_connected(net, self.n_classes,[m
                                       activation_fn=None,[m
